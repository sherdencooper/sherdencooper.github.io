---
---
@INPROCEEDINGS{yang2021matrix,
bibtex_show={true},
abbr={TMC},
author = {Jungang Yang and Liyao Xiang and Jiahao Yu and Xinbing Wang and Bin Guo and Zhetao Li and Baochun Li},
booktitle = {IEEE Transactions on Mobile Computing},
title = {Matrix Gaussian Mechanisms for Differentially-Private Learning},
year = {2021},
pdf = {https://ieeexplore.ieee.org/document/9475590},
abstract={The wide deployment of machine learning algorithms has become a severe threat to user data privacy. As the learning data is of high dimensionality and high orders, preserving its privacy is intrinsically hard. Conventional differential privacy mechanisms often incur significant utility decline as they are designed for scalar values from the start. We recognize that it is because conventional approaches do not take the data structural information into account, and fail to provide sufficient privacy or utility. As the main novelty of this work, we propose Matrix Gaussian Mechanism (MGM), a new (, δ)-differential privacy mechanism for preserving learning data privacy. By imposing the unimodal distributions on the noise, we introduce two mechanisms based on MGM with an improved utility. We further show that with the utility space available, the proposed mechanisms can be instantiated with optimized utility, and has a closed-form solution scalable to large-scale problems. We experimentally show that our mechanisms, applied to privacy-preserving federated learning, are superior than the state-of-the-art differential privacy mechanisms in utility.}
}

@inproceedings{statemask,
  abbr={NIPS},
  bibtex_show={false},
  title={StateMask: Explaining Deep Reinforcement Learning through State Mask},
  author={Cheng*, Zelei and Wu*, Xian and Yu*, Jiahao and Sun, Wenhai and Guo, Wenbo and Xing, Xinyu},
  booktitle={Proceedings of the 37th Conference on Neural Information Processing Systems},
  pdf={https://openreview.net/pdf?id=pzc6LnUxYN},
  year={2023}
}

@inproceedings{cheng2024rice,
  title={RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation},
  abbr={ICML},
  author={Cheng, Zelei and Wu, Xian and Yu, Jiahao and Yang, Sabrina and Wang, Gang and Xing, Xinyu},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  pdf = {https://arxiv.org/pdf/2405.03064}
  year={2024}
}

@inproceedings{gptfuzzer,
  abbr={arXiv},
  title={GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts },
  author={Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu},
  booktitle={arXiv preprint arXiv:2309.10253},
  year={2023},
  pdf = {https://arxiv.org/pdf/2309.10253.pdf}
}

@inproceedings{yu2023assessing,
  abbr={ICLR@SET-LLM},
  title={Assessing Prompt Injection Risks in 200+ Custom GPTs},
  author={Yu, Jiahao and Wu, Yuhang and Shu, Dong and Jin, Mingyu and Yang, Sabrina and Xing, Xinyu},
  booktitle={ICLR 2024 Workshop on Secure and Trustworthy Large Language Models},
  year={2024},
  pdf = {https://openreview.net/pdf?id=XoXktpKp6x}
}

@inproceedings{jiang2020research,
  abbr={J Phys Conf Ser},
  title={Research on Application of Artificial Intelligence Technology in Electrical Automation Control},
  author={Jiang, Chao and Xiong, Xiaorui and Zhu, Tanqing and Cao, Jiajia and Yu, Jiahao},
  booktitle={Journal of Physics: Conference Series},
  year={2020},
}

@inproceedings{xu2021speedup,
  abbr={CIKM},
  bibtex_show={false},
  title={Speedup robust graph structure learning with low-rank information},
  author={Xu, Hui and Xiang, Liyao and Yu, Jiahao and Cao, Anqi and Wang, Xinbing},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={2241--2250},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3459637.3482299},
  year={2021}
}

@inproceedings{zhang2020voiceprint,
  abbr={INFOCOM},
  bibtex_show={true},
  title={Voiceprint mimicry attack towards speaker verification system in smart home},
  author={Zhang, Lei and Meng, Yan and Yu, Jiahao and Xiang, Chong and Falk, Brandon and Zhu, Haojin},
  booktitle={Proceedings of IEEE INFOCOM},
  pages={377--386},
  year={2020},
  organization={IEEE},
  abstract={The advancement of voice controllable systems (VC-Ses) has dramatically affected our daily lifestyle and catalyzed the smart home's deployment. Currently, most VCSes exploit automatic speaker verification (ASV) to prevent various voice attacks (e.g., replay attack). In this study, we present VMask, a novel and practical voiceprint mimicry attack that could fool ASV in smart home and inject the malicious voice command disguised as a legitimate user. The key observation behind VMask is that the deep learning models utilized by ASV are vulnerable to the subtle perturbations in the voice input space. To generate these subtle perturbations, VMask leverages the idea of adversarial examples. Then by adding the subtle perturbations to the recordings from an arbitrary speaker, VMask can mislead the ASV into classifying the crafted speech samples, which mirror the former speaker for human, as the targeted victim. Moreover, psychoacoustic masking is employed to manipulate the adversarial perturbations under human perception threshold, thus making victim unaware of ongoing attacks. We validate the effectiveness of VMask by performing comprehensive experiments on both grey box (VGGVox) and black box (Microsoft Azure Speaker Verification) ASVs. Additionally, a real-world case study on Apple HomeKit proves the VMask's practicability on smart home platforms.},
  pdf = {https://ieeexplore.ieee.org/document/9155483}
}

@inproceedings{li2019invisible,
  abbr={arXiv},
  title={Invisible backdoor attacks against deep neural networks},
  author={Li, Shaofeng and Zhao, Benjamin Zi Hao and Yu, Jiahao and Xue, Minhui and Kaafar, Dali and Zhu, Haojin},
  booktitle={arXiv preprint arXiv:1909.02742v1},
  year={2019},
  pdf = {https://arxiv.org/pdf/1909.02742v1.pdf}
}


@INPROCEEDINGS{airs2023,
  abbr={USENIX},
  title={AIRS Explanation for Deep Reinforcement Learning based Security Applications},
  author={Jiahao Yu and  Wenbo Guo and  Qi Qin and Gang Wang and Ting Wang and Xinyu Xing},
  booktitle = {Proceedings of the 2023 USENIX Security},
  abstract = {Recently, we have witnessed the success of deep reinforcement learning (DRL) in many security applications, ranging from malware mutation to selfish blockchain mining. Like all other machine learning methods, the lack of explainability has been limiting its broad adoption as users have difficulty establishing trust in DRL models’ decisions. Over the past years, different methods have been proposed to explain DRL models but unfortunately, they are often not suitable for security applications, in which explanation fidelity, efficiency, and the capability of model debugging are largely lacking. In this work, we propose AIRS, a general framework to explain deep reinforcement learning-based security applications. Unlike previous works that pinpoint important features to the agent’s current action, our explanation is at the step level. It models the relationship between the final reward and the key steps that a DRL agent takes, and thus outputs the steps that are most critical towards the final reward the agent has gathered. Using four representative security-critical applications, we evaluate AIRS from the perspectives of explainability, fidelity, stability, and efficiency. We show that AIRS could outperform alternative explainable DRL methods. We also showcase AIRS’s utility, demonstrating that our explanation could facilitate the DRL model’s failure offset, help users establish trust in a model decision, and even assist the identification of inappropriate reward designs.},
  year = {2023},
  pdf = {airs.pdf},
  selected = {true}
}


@INPROCEEDINGS{shi2024bandfuzz,
  abbr={ICSE@SBFT},
  title={BandFuzz: A Practical Framework for Collaborative Fuzzing with Reinforcement Learning},
  author={Shi, Wenxuan and Li, Hongwei and Yu, Jiahao and Guo, Wenbo and Xing, Xinyu},
  year = {2024},
  booksubtitle = {The 17th Intl. Workshop on Search-Based and Fuzz Testing},
  pdf = {https://shiwx.org/paper/bandfuzz-sbft24.pdf},
}