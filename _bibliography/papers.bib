---
---
@article{yu2025building,
  abbr={arXiv},
  title={Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization},
  author={Yu*, Jiahao and Cheng*, Zelei and Wu, Xian and Xing, Xinyu},
  journal={arXiv preprint arXiv:2509.12434},
  year={2026},
  pdf = {https://arxiv.org/pdf/2509.12434},
  selected = {true},
  highlight = {true},
  highlight_content = {1st on SWEBench-Lite(Open-weight)</br>5th on SWEBench-Verified(Open-weight)}
}

@article{zhu2025locus,
  abbr={arXiv},
  title={Locus: Agentic Predicate Synthesis for Directed Fuzzing},
  author={Zhu, Jie and Shen, Chihao and Li, Ziyang and Yu, Jiahao and Chen, Yizheng and Pei, Kexin},
  journal={arXiv preprint arXiv:2508.21302},
  year={2026}
}

@inproceedings{yu2025gpo,
  abbr={NIPS},
  title={GPO: Learning from Critical Steps to Improve LLM Reasoning},
  author={Yu*, Jiahao and Cheng, Zelei and Wu, Xian and Xing, Xinyu},
  journal={Proceedings of the 39th Conference on Neural Information Processing Systems},
  year={2025},
  pdf = {https://arxiv.org/pdf/2509.16456},
  selected = {true}
}

@inproceedings{yu2024blockfound,
  abbr={NIPS},
  title={BlockScan: Detecting Anomalies in Blockchain Transactions},
  author={Yu*, Jiahao and Wu*, Xian and Liu, Hao and Guo, Wenbo and Xing, Xinyu},
  journal={Proceedings of the 39th Conference on Neural Information Processing Systems},
  year={2025},
  pdf = {https://arxiv.org/abs/2410.04039},
  selected = {true}
}

@inproceedings{yu2024enhancing,
  abbr={USENIX},
  title={Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs’ Ethical Boundaries},
  author={Yu*, Jiahao and Luo*, Haozheng and Yao-Chieh, Jerry and Guo, Wenbo and Liu, Han and Xing, Xinyu},
  booktitle={Proceedings of the 2025 USENIX Security},
  year={2025},
  pdf = {https://arxiv.org/pdf/2405.20653},
  selected = {true},
  highlight = {true},
  highlight_content = {Long Talk},
}

@inproceedings{yu2025patchagent,
  abbr={USENIX},
  title={PATCHAGENT: A Practical Program Repair Agent Mimicking Human Expertise},
  author={Yu, Zheng and Guo, Ziyi and Wu, Yuhang and Yu, Jiahao and Xu, Meng and Mu, Dongliang and Chen, Yan and Xing, Xinyu},
  booktitle={Proceedings of the 2025 USENIX Security},
  pages={4381--4400},
  year={2025},
  selected = {true},
  highlight = {true},
  highlight_content = {Long Talk, Patched over 10 real-world bugs},
}

@inproceedings{wangpft,
  title={The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)},
  abbr={ICML},
  author={Wang, Zihao and Jiang, Yibo and Yu, Jiahao and Huang, Heqing},
  booktitle={Proceedings of the 42nd International Conference on Machine Learning},
  year={2025},
  selected = {true}
}

@inproceedings{cai2024utf,
  abbr={ACL@LLMSEC},
  title={UTF: Undertrained Tokens as Fingerprints A Novel Approach to LLM Identification},
  author={Cai*, Jiacheng and Yu*, Jiahao and Shao, Yangguang and Wu, Yuhang and Xing, Xinyu},
  journal={arXiv preprint arXiv:2410.12318},
  year={2025},
  pdf = {https://arxiv.org/pdf/2410.12318}
}

@inproceedings{luo2025knowledge,
  abbr={ICML@MemFM},
  title={Knowledge-Distilled Memory Editing for Plug-and-Play LLM Alignment},
  author={Luo, Haozheng and Yu, Jiahao and Zhang, Wenxin and Li, Jialong and Hu, Jerry Yao-Chieh and Chen, Yan and Wang, Binghui and Xing, Xinyu and Liu, Han},
  booktitle={The Impact of Memorization on Trustworthy Foundation Models: ICML 2025 Workshop},
  year={2025}
}

@article{shao2025poisoncraft,
  abbr={arXiv},
  title={PoisonCraft: Practical Poisoning of Retrieval-Augmented Generation for Large Language Models},
  author={Shao, Yangguang and Lin, Xinjie and Luo, Haozheng and Hou, Chengshang and Xiong, Gang and Yu, Jiahao and Shi, Junzheng},
  journal={arXiv preprint arXiv:2505.06579},
  year={2025}
}

@article{cheng2025survey,
  abbr={arXiv},
  title={A survey on explainable deep reinforcement learning},
  author={Cheng, Zelei and Yu, Jiahao and Xing, Xinyu},
  journal={arXiv preprint arXiv:2502.06869},
  year={2025}
}

@article{luo2025genoarmory,
  abbr={arXiv},
  title={GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models},
  author={Luo, Haozheng and Qiu, Chenghao and Wang, Yimin and Wu, Shang and Yu, Jiahao and Liu, Han and Wang, Binghui and Chen, Yan},
  journal={arXiv preprint arXiv:2505.10983},
  year={2025}
}

@article{shi2025bandfuzz,
  abbr={arXiv},
  title={BandFuzz: An ML-powered Collaborative Fuzzing Framework},
  author={Shi, Wenxuan and Li, Hongwei and Yu, Jiahao and Sun, Xinqian and Guo, Wenbo and Xing, Xinyu},
  journal={arXiv preprint arXiv:2507.10845},
  year={2025}
}

@INPROCEEDINGS{fuzzer2024,
  abbr={USENIX},
  title={LLM-Fuzzer: Scaling Assessment of Large Language Model Jailbreaks},
  author={Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu},
  booktitle = {Proceedings of the 2024 USENIX Security},
  year = {2024},
  selected = {true}
}

@inproceedings{cheng2024soft,
  abbr={NIPS},
  title={Soft-Label Integration for Robust Toxicity Classification},
  booktitle={Proceedings of the 38th Conference on Neural Information Processing Systems},
  author={Cheng, Zelei and Wu, Xian and Yu, Jiahao and Han, Shuo and Cai, Xin-Qiang and Xing, Xinyu},
  journal={arXiv preprint arXiv:2410.14894},
  pdf = {https://arxiv.org/pdf/2410.14894},
  year={2024},
  selected = {true},
  highlight = {true},
  highlight_content = {Featured in MIT Technology Review China},
}

@inproceedings{cheng2024rice,
  title={RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation},
  abbr={ICML},
  author={Cheng, Zelei and Wu, Xian and Yu, Jiahao and Yang, Sabrina and Wang, Gang and Xing, Xinyu},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  selected = {true},
  highlight = {true},
  highlight_content = {Spotlight Top-3.5%},
  year={2024}
}

@inproceedings{shi2024bandfuzz,
  abbr={ICSE@SBFT},
  title={BandFuzz: A Practical Framework for Collaborative Fuzzing with Reinforcement Learning},
  author={Shi, Wenxuan and Li, Hongwei and Yu, Jiahao and Guo, Wenbo and Xing, Xinyu},
  booktitle = {The 17th Intl Workshop on Search-Based and Fuzz Testing},
  year = {2024},
  pdf = {https://shiwx.org/paper/bandfuzz-sbft24.pdf},
  highlight = {true},
  highlight_content = {1st Place in SBFT Challenge}
}

@inproceedings{yu2023assessing,
  abbr={ICLR@SET-LLM},
  title={Assessing Prompt Injection Risks in 200+ Custom GPTs},
  author={Yu, Jiahao and Wu, Yuhang and Shu, Dong and Jin, Mingyu and Yang, Sabrina and Xing, Xinyu},
  booktitle={ICLR 2024 Workshop on Secure and Trustworthy Large Language Models},
  year={2024},
  pdf = {https://openreview.net/pdf?id=XoXktpKp6x},
  selected = {true},
  highlight = {true},
  highlight_content = {Featured in WIRED},
}

@inproceedings{yu2024promptfuzz,
  abbr={arXiv},
  title={PromptFuzz: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs},
  author={Yu*, Jiahao and Shao*, Yangguang and Miao*, Hanwen and Shi, Junzheng and Xing, Xinyu},
  journal={arXiv preprint arXiv:2409.14729},
  year={2024},
  pdf = {https://arxiv.org/pdf/2409.14729}
}

@inproceedings{luo2024decoupled,
  abbr={arXiv},
  title={Decoupled Alignment for Robust Plug-and-Play Adaptation},
  author={Luo*, Haozheng and Yu*, Jiahao and Zhang, Wenxin and Li, Jialong and Hu, Jerry Yao-Chieh and Xin, Xingyu and Liu, Han},
  journal={arXiv preprint arXiv:2406.01514},
  year={2024},
  pdf = {https://arxiv.org/pdf/2406.01514}
}

@inproceedings{gptfuzzer,
  abbr={arXiv},
  title={GPTFuzzer: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts },
  author={Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu},
  journal={arXiv preprint arXiv:2309.10253},
  year={2023},
  pdf = {https://arxiv.org/pdf/2309.10253.pdf},
  selected = {true},
  highlight = {true},
  highlight_content = {Geekcon 2023 Annual Themed Debate Breakthrough Awards},
}
@inproceedings{statemask,
  abbr={NIPS},
  bibtex_show={false},
  title={StateMask: Explaining Deep Reinforcement Learning through State Mask},
  author={Cheng*, Zelei and Wu*, Xian and Yu*, Jiahao and Sun, Wenhai and Guo, Wenbo and Xing, Xinyu},
  booktitle={Proceedings of the 37th Conference on Neural Information Processing Systems},
  pdf={https://openreview.net/pdf?id=pzc6LnUxYN},
  year={2023},
  selected = {true}
}

@INPROCEEDINGS{airs2023,
  abbr={USENIX},
  title={AIRS Explanation for Deep Reinforcement Learning based Security Applications},
  author={Jiahao Yu and  Wenbo Guo and  Qi Qin and Gang Wang and Ting Wang and Xinyu Xing},
  booktitle = {Proceedings of the 2023 USENIX Security},
  abstract = {Recently, we have witnessed the success of deep reinforcement learning (DRL) in many security applications, ranging from malware mutation to selfish blockchain mining. Like all other machine learning methods, the lack of explainability has been limiting its broad adoption as users have difficulty establishing trust in DRL models’ decisions. Over the past years, different methods have been proposed to explain DRL models but unfortunately, they are often not suitable for security applications, in which explanation fidelity, efficiency, and the capability of model debugging are largely lacking. In this work, we propose AIRS, a general framework to explain deep reinforcement learning-based security applications. Unlike previous works that pinpoint important features to the agent’s current action, our explanation is at the step level. It models the relationship between the final reward and the key steps that a DRL agent takes, and thus outputs the steps that are most critical towards the final reward the agent has gathered. Using four representative security-critical applications, we evaluate AIRS from the perspectives of explainability, fidelity, stability, and efficiency. We show that AIRS could outperform alternative explainable DRL methods. We also showcase AIRS’s utility, demonstrating that our explanation could facilitate the DRL model’s failure offset, help users establish trust in a model decision, and even assist the identification of inappropriate reward designs.},
  year = {2022},
  pdf = {https://www.usenix.org/system/files/usenixsecurity23-yu-jiahao.pdf},
  selected = {true}
}

@INPROCEEDINGS{yang2021matrix,
bibtex_show={true},
abbr={TMC},
author = {Jungang Yang and Liyao Xiang and Jiahao Yu and Xinbing Wang and Bin Guo and Zhetao Li and Baochun Li},
booktitle = {IEEE Transactions on Mobile Computing},
title = {Matrix Gaussian Mechanisms for Differentially-Private Learning},
year = {2021},
pdf = {https://ieeexplore.ieee.org/document/9475590},
abstract={The wide deployment of machine learning algorithms has become a severe threat to user data privacy. As the learning data is of high dimensionality and high orders, preserving its privacy is intrinsically hard. Conventional differential privacy mechanisms often incur significant utility decline as they are designed for scalar values from the start. We recognize that it is because conventional approaches do not take the data structural information into account, and fail to provide sufficient privacy or utility. As the main novelty of this work, we propose Matrix Gaussian Mechanism (MGM), a new (, δ)-differential privacy mechanism for preserving learning data privacy. By imposing the unimodal distributions on the noise, we introduce two mechanisms based on MGM with an improved utility. We further show that with the utility space available, the proposed mechanisms can be instantiated with optimized utility, and has a closed-form solution scalable to large-scale problems. We experimentally show that our mechanisms, applied to privacy-preserving federated learning, are superior than the state-of-the-art differential privacy mechanisms in utility.}
}

@inproceedings{xu2021speedup,
  abbr={CIKM},
  bibtex_show={false},
  title={Speedup robust graph structure learning with low-rank information},
  author={Xu, Hui and Xiang, Liyao and Yu, Jiahao and Cao, Anqi and Wang, Xinbing},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={2241--2250},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3459637.3482299},
  year={2021}
}

@inproceedings{zhang2020voiceprint,
  abbr={INFOCOM},
  bibtex_show={true},
  title={Voiceprint mimicry attack towards speaker verification system in smart home},
  author={Zhang, Lei and Meng, Yan and Yu, Jiahao and Xiang, Chong and Falk, Brandon and Zhu, Haojin},
  booktitle={Proceedings of IEEE INFOCOM},
  pages={377--386},
  year={2020},
  organization={IEEE},
  abstract={The advancement of voice controllable systems (VC-Ses) has dramatically affected our daily lifestyle and catalyzed the smart home's deployment. Currently, most VCSes exploit automatic speaker verification (ASV) to prevent various voice attacks (e.g., replay attack). In this study, we present VMask, a novel and practical voiceprint mimicry attack that could fool ASV in smart home and inject the malicious voice command disguised as a legitimate user. The key observation behind VMask is that the deep learning models utilized by ASV are vulnerable to the subtle perturbations in the voice input space. To generate these subtle perturbations, VMask leverages the idea of adversarial examples. Then by adding the subtle perturbations to the recordings from an arbitrary speaker, VMask can mislead the ASV into classifying the crafted speech samples, which mirror the former speaker for human, as the targeted victim. Moreover, psychoacoustic masking is employed to manipulate the adversarial perturbations under human perception threshold, thus making victim unaware of ongoing attacks. We validate the effectiveness of VMask by performing comprehensive experiments on both grey box (VGGVox) and black box (Microsoft Azure Speaker Verification) ASVs. Additionally, a real-world case study on Apple HomeKit proves the VMask's practicability on smart home platforms.},
  pdf = {https://ieeexplore.ieee.org/document/9155483}
}

@inproceedings{jiang2020research,
  abbr={J Phys Conf Ser},
  title={Research on Application of Artificial Intelligence Technology in Electrical Automation Control},
  author={Jiang, Chao and Xiong, Xiaorui and Zhu, Tanqing and Cao, Jiajia and Yu, Jiahao},
  booktitle={Journal of Physics: Conference Series},
  year={2020},
}

@inproceedings{li2019invisible,
  abbr={arXiv},
  title={Invisible backdoor attacks against deep neural networks},
  author={Li, Shaofeng and Zhao, Benjamin Zi Hao and Yu, Jiahao and Xue, Minhui and Kaafar, Dali and Zhu, Haojin},
  journal={arXiv preprint arXiv:1909.02742v1},
  year={2019},
  pdf = {https://arxiv.org/pdf/1909.02742v1.pdf}
}